import argparse
import os
import json
from bottle import Bottle, request, response, run, static_file
import bottle
from paste import httpserver
import time
import threading
from torch.multiprocessing import Process, Queue, set_start_method, Event
import torch
from .factcheckers import FactChecker
from .qa_models import QAModel
from .get_example import ExampleGetter
import spacy

bottle.BaseRequest.MEMFILE_MAX = 10240000

web_root = f"{os.path.dirname(__file__)}/webroot/"
samples_path = f"{os.path.dirname(__file__)}/examples/saved"


def consumer_procroot(
    pidx, cls, args_dict, in_queue: Queue, res_queue: Queue, init_event: Event
):
    fc = cls(**args_dict)
    init_event.set()

    while True:
        inp = in_queue.get(block=True)
        key = inp["key"]
        payload = inp["payload"]
        result = fc.predict(**payload)
        had_success = result["success"]
        if not had_success:
            print("WARNING: FAILED A PREDICTION")

        res_queue.put({"key": key, "payload": result})


def manager_threadroot(lockdict, res_queue: Queue, results_dict):
    while True:
        out = res_queue.get(block=True)
        key = out["key"]
        results_dict[key] = out["payload"]
        assert key in lockdict
        event = lockdict[key]
        event.set()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="frontend server for genaudit")

    parser.add_argument(
        "--bind", type=str, default="localhost", help="the interface to bind to"
    )
    parser.add_argument(
        "--port", type=int, required=True, help="port to use for listening to requests"
    )
    parser.add_argument(
        "--qa-model",
        type=str,
        default="",
        help="model to use for answering questions (optional)",
    )
    parser.add_argument(
        "--factcheck-model",
        type=str,
        required=True,
        help="model to use for fact-checking claims",
    )
    parser.add_argument(
        "--num-factcheck-processes",
        type=int,
        default=1,
        help="can spawn multiple models for factchecking sentences in parallel. useful if you have multiple gpus.",
    )
    parser.add_argument(
        "--max-doc-words",
        type=int,
        default=1500,
        help="maximum number of words allowed in the input. note that no truncation happens when doing fact-checking or QA. Set according to available GPU memory.",
    )
    parser.add_argument(
        "--fc-max-decode-len",
        type=int,
        default=250,
        help="maximum output length for the fact-checking model. should be set to around the expected maximum length of a sentence being factchecked.",
    )
    parser.add_argument(
        "--fc-nbeams",
        type=int,
        default=4,
        help="number of beams to use while decoding with the fact-checking model",
    )
    parser.add_argument(
        "--qa-max-decode-len",
        type=int,
        default=500,
        help="maximum number of tokens to generate while answering questions with the QA model",
    )
    parser.add_argument(
        "--qa-dosample",
        action="store_true",
        help="whether to use sampling while generating response from the QA model",
    )
    parser.add_argument(
        "--qa-temperature",
        type=float,
        default=1.0,
        help="temperature used while sampling from the QA model",
    )
    parser.add_argument(
        "--qa-nbeams",
        type=int,
        default=1,
        help="number of beams to use while decoding from the QA model",
    )
    parser.add_argument(
        "--qa-top-p",
        type=float,
        default=None,
        help="the value of p in the top-p sampling procedure with the QA model",
    )
    parser.add_argument(
        "--qa-quantize",
        type=str,
        default="16bit",
        help="quantization to use for the QA model (should be one of: 16bit/8bit/4bit)",
    )
    parser.add_argument(
        "--use-single-gpu",
        action="store_true",
        help="if you want all models to be loaded on the same GPU, use this flag. Otherwise, each model is loaded on a different GPU.",
    )
    parser.add_argument(
        "--save-path",
        type=str,
        default="",
        help="path to a directory for saving data (reference doc, questions, and responses after potential editing).",
    )
    parser.add_argument(
        "--distributed",
        action="store_true",
        help="Use FactChecker with HF accelerate's  distributed inference",
    )

    args = parser.parse_args()

    set_start_method("spawn")

    ex_getter = ExampleGetter(samples_path=samples_path, save_path=args.save_path)
    app = Bottle()

    try:
        nlp = spacy.load("en_core_web_md")
    except:
        # if model not found then download it
        from spacy.cli import download

        download("en_core_web_md")
        nlp = spacy.load("en_core_web_md")

    gpu_counter = 0

    input_queue = Queue()
    result_queue = Queue()
    results_dict = {}

    fc_towait_events = []

    for pidx in range(args.num_factcheck_processes):
        init_event = Event()
        fc_towait_events.append(init_event)
        constructor_args = {
            "model_name": args.factcheck_model,
            "distributed": args.distributed,
            "gpu_idx": gpu_counter,
            "nbeams": args.fc_nbeams,
            "max_decode_len": args.fc_max_decode_len,
        }
        proc = torch.multiprocessing.Process(
            target=consumer_procroot,
            args=(
                pidx,
                FactChecker,
                constructor_args,
                input_queue,
                result_queue,
                init_event,
            ),
        )
        proc.start()
        if not args.use_single_gpu:
            gpu_counter += 1

    [ev.wait() for ev in fc_towait_events]
    print("Fact-checking models started. üèÅ")

    lockdict = {}
    manager_thread = threading.Thread(
        target=manager_threadroot, args=(lockdict, result_queue, results_dict)
    )
    manager_thread.start()

    qa_model_available = args.qa_model != ""

    input_queue2 = Queue()
    result_queue2 = Queue()
    results_dict2 = {}
    lockdict2 = {}
    init_event2 = Event()

    if qa_model_available:
        constructor_args = {
            "model_name": args.qa_model,
            "gpu_idx": gpu_counter,
            "temperature": args.qa_temperature,
            "top_p": args.qa_top_p,
            "dosample": args.qa_dosample,
            "max_decode_len": args.qa_max_decode_len,
            "quantize": args.qa_quantize,
            "nbeams": args.qa_nbeams,
        }
        proc2 = torch.multiprocessing.Process(
            target=consumer_procroot,
            args=(
                0,
                QAModel,
                constructor_args,
                input_queue2,
                result_queue2,
                init_event2,
            ),
        )
        proc2.start()
        manager_thread2 = threading.Thread(
            target=manager_threadroot, args=(lockdict2, result_queue2, results_dict2)
        )
        manager_thread2.start()

        init_event2.wait()
        print("QA model started. üèÅ")

    @app.hook("after_request")
    def enable_cors():
        """
        You need to add some headers to each request.
        Don't use the wildcard '*' for Access-Control-Allow-Origin in production.
        """
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = (
            "PUT, GET, POST, DELETE, OPTIONS"
        )
        response.headers["Access-Control-Allow-Headers"] = (
            "Origin, Accept, Content-Type, X-Requested-With, X-CSRF-Token"
        )

    @app.route("/")
    def serve_job():
        html_str = open(os.path.join(web_root, "index.html")).read()
        return html_str

    @app.route("/get_config", method=["GET"])
    def get_config():
        return {
            "factcheck_model": {
                "model_name_or_path": args.factcheck_model,
                "num_procs": args.num_factcheck_processes,
            },
            "qa_model": {
                "model_name_or_path": args.qa_model,
                "quantization": args.qa_quantize,
                "max_decode_len": args.qa_max_decode_len,
            },
            "max_doc_words": args.max_doc_words,
            "save_path": args.save_path,
        }

    @app.route("/get_all_ids", method=["GET"])
    def get_all_ids():
        output = [
            {"label": x, "val": j} for (j, x) in enumerate(ex_getter.get_all_ids())
        ]
        return {"all_ids": output}

    @app.route("/static/<filename:path>")
    def send_static(filename):
        return static_file(filename, root=web_root)

    @app.route("/get_example/<jobid>")
    def get_example(jobid):
        jobid = int(jobid)
        one_dp = ex_getter.get_article(jobid)

        return_obj_formatted = {}
        return_obj_formatted["job_id"] = one_dp["id"]
        return_obj_formatted["input_lines"] = one_dp["input_lines"]
        return_obj_formatted["output_lines"] = one_dp["output_lines"]

        for i in range(len(return_obj_formatted["input_lines"])):
            return_obj_formatted["input_lines"][i] = return_obj_formatted[
                "input_lines"
            ][i].strip()

        for i in range(len(return_obj_formatted["output_lines"])):
            return_obj_formatted["output_lines"][i] = return_obj_formatted[
                "output_lines"
            ][i].strip()

        if "question" in one_dp:
            return_obj_formatted["question"] = one_dp["question"]

        return return_obj_formatted

    @app.route("/get_qa", method=["POST"])
    def get_qa():
        if not qa_model_available:
            return {"success": False, "reason": "QA model not running."}

        bundle = request.forms.get("bundle")
        bytes_string = bytes(bundle, encoding="raw_unicode_escape")
        bundle = bytes_string.decode("utf-8", "strict")
        bundle = json.loads(bundle)

        article_lines = bundle["article_lines"]
        question = bundle["question"]
        article_lines = [x["txt"] for x in article_lines]

        send_dp = {"document": article_lines, "question": question}

        curtime = str(time.time())
        event = threading.Event()
        lockdict2[curtime] = event
        input_queue2.put({"key": curtime, "payload": send_dp})

        event.wait()

        del lockdict2[curtime]
        recv_pred = results_dict2.pop(curtime)
        qa_output = recv_pred["result"]

        qa_output = qa_output.replace("\n", " ").strip()
        doc = nlp(qa_output)
        sents = [str(s) for s in doc.sents]

        return {"success": True, "prediction": sents}

    @app.route("/get_ev_with_fixfactuality", method=["POST"])
    def get_factuality():
        bundle = request.forms.get("bundle")
        bytes_string = bytes(bundle, encoding="raw_unicode_escape")
        bundle = bytes_string.decode("utf-8", "strict")
        bundle = json.loads(bundle)

        article_lines = bundle["article_lines"]
        summary_line = bundle["summary_line"]
        prev_lines = bundle["prev_lines"]

        article_lines = [x["txt"] for x in article_lines]

        send_dp = {
            "reference_sents": article_lines,
            "claim": summary_line,
            "prev_sents": prev_lines,
        }

        curtime = str(time.time())
        event = threading.Event()
        lockdict[curtime] = event
        input_queue.put({"key": curtime, "payload": send_dp})

        event.wait()

        del lockdict[curtime]
        recv_pred = results_dict.pop(curtime)
        fc_output = recv_pred["result"]

        return fc_output

    @app.route("/save_example", method=["POST"])
    def save_example():
        if args.save_path == "":
            return {
                "success": False,
                "reason": "The path to save examples has not been declared. Aborting...",
            }

        bundle_str = request.forms.get("bundle")
        bytes_string = bytes(bundle_str, encoding="raw_unicode_escape")
        bundle_str = bytes_string.decode("utf-8", "strict")

        save_obj = json.loads(bundle_str)

        _id = save_obj["id"]

        output_fpath = f"{args.save_path}/{_id}.json"

        if os.path.exists(output_fpath):
            return {"success": False, "reason": "Object already exists with that ID"}

        else:
            with open(output_fpath, "w", encoding="utf-8") as w:
                json.dump(save_obj, w, ensure_ascii=False)
            return {"success": True, "reason": "Object saved successfully"}

    @app.route("/sent_tokenize", method=["POST"])
    def sent_tokenize():
        new_doc = request.forms.get("doc")
        bytes_string = bytes(new_doc, encoding="raw_unicode_escape")
        new_doc = bytes_string.decode("utf-8", "strict")
        new_doc = new_doc.strip()
        new_doc = new_doc.replace("\n", " ")
        doc = nlp(new_doc)
        sents = [str(s) for s in doc.sents]

        newtxt = "\n".join(sents)

        return {"prediction": newtxt}

    @app.route("/check_length", method=["POST"])
    def check_length():
        new_doc = request.forms.get("doc")
        bytes_string = bytes(new_doc, encoding="raw_unicode_escape")
        new_doc = bytes_string.decode("utf-8", "strict")
        new_doc = new_doc.strip()
        new_doc = new_doc.replace("\n", " ")
        doc = nlp(new_doc)
        curr_len = len(doc)

        to_return = {
            "curr_len": curr_len,
            "allowed_len": args.max_doc_words,
            "okay": curr_len <= args.max_doc_words,
        }

        return to_return

    httpserver.serve(app, host=args.bind, port=args.port)
